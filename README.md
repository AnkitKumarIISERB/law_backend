# ğŸ§  Law Assistant Backend (Flask + Groq + Sentence Transformers)

This is the backend for a legal assistant application built with Flask and powered by Groq's large language models. It uses sentence embeddings to retrieve the most relevant legal section from Indian law and generates a human-friendly explanation using an LLM.

---

## ğŸš€ Features

- ğŸ§¾ Semantic search over Indian legal sections (BNS)
- ğŸ”— Sentence embeddings with `sentence-transformers`
- ğŸ§  Answers generated via Groq's `gemma2-9b-it` LLM
- ğŸ”’ `.env`-based API key configuration
- ğŸ”¥ Flask + CORS support for smooth frontend integration

---

## ğŸ—‚ï¸ Project Structure

law_backend/
â”œâ”€â”€ app.py # Main Flask app
â”œâ”€â”€ bns.txt # Legal sections (raw text)
â”œâ”€â”€ bns_embeddings.pkl # Precomputed embeddings for fast search
â”œâ”€â”€ generate_embeddings.ipynb # Colab notebook to generate embeddings
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # You're reading this!

---

## âš™ï¸ Environment Variables

Create a `.env` file in the root folder with your Groq API key:

env
GROQ_API_KEY=`your_groq_api_key_here`

---

## ğŸ““ Embedding Generation

The embeddings (bns_embeddings.pkl) were created using the notebook generate_embeddings.ipynb. It uses the all-MiniLM-L6-v2 model from sentence-transformers to encode each section from bns.txt.

---

## ğŸ§ª API Usage
POST /ask
Request Body:
{
  "query": "What is the punishment for theft?"
}
Response:
{
  "section": "Section text that is most relevant",
  "answer": "LLM-generated explanation based on section"
}

---

## ğŸ§© Dependencies
Install all required packages:
pip install -r requirements.txt

---

## ğŸ›°ï¸ Related Repositories
ğŸ‘‰ https://github.com/AnkitKumarIISERB/law_frontend

---

## ğŸ“œ License
This project is licensed under the MIT License.

---





